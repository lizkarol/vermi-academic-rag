# Gu√≠a de Contribuci√≥n - Vermi Academic RAG

¬°Gracias por tu inter√©s en contribuir! Este proyecto sigue un modelo **BYOS (Bring Your Own Sources)** estricto para cumplir con derechos de autor.

---

## üìú Pol√≠tica BYOS (Bring Your Own Sources)

**Este repositorio NO aloja PDFs ni Markdown derivados de terceros con copyright.**

### ‚úÖ Qu√© S√ç puedes aportar:

- **Chunks JSONL parafraseados** (en tus propias palabras, NO copias verbatim)
- **C√≥digo Python** (scripts, tests, mejoras)
- **Documentaci√≥n** (gu√≠as, ejemplos, FAQs)
- **Referencias** (DOI/URLs de fuentes, sin los archivos)

### ‚ùå Qu√© NO debes aportar:

- PDFs o archivos originales con copyright
- Markdown sin parafrasear (conversi√≥n directa)
- Fragmentos verbatim >100 palabras consecutivas
- Contenido sin atribuci√≥n clara

**M√°s detalles:** Ver [`BYOS_POLICY.md`](BYOS_POLICY.md)

---

## üîÑ Flujo de Trabajo BYOS Completo

### Fase 1: Conversi√≥n Local (PDF ‚Üí Markdown)

**Objetivo:** Obtener un Markdown de alta fidelidad de tu fuente acad√©mica.

```bash
# 1. Aseg√∫rate de tener el entorno configurado
source venv/bin/activate  # macOS/Linux
# .\venv\Scripts\Activate.ps1  # Windows

# 2. Convierte tu PDF (permanece local, NO se sube)
python scripts/conversion/adaptive_converter.py "ruta/a/tu/paper.pdf"

# Salida: sources_local/markdown_outputs/tu_paper.md
```

**Puntos clave:**
- Este paso usa `marker-sdk` para preservar tablas, figuras y ecuaciones
- El MD generado es tu "materia prima" para el siguiente paso
- **Permanece local**: `sources_local/` est√° en `.gitignore`

### Fase 2: Generaci√≥n de Chunks (Markdown ‚Üí JSONL)

**Objetivo:** Convertir el contenido del MD en chunks parafraseados estructurados.

**Proceso Manual con LLM:**

1. **Abre el prompt template:**
   ```bash
   cat scripts/chunking/generate_cards_local.md
   ```

2. **Copia el contenido del Markdown generado** en el Paso 1

3. **Usa tu LLM favorito:**
   - Gemini Pro (recomendado para espa√±ol)
   - GPT-4 / GPT-4o
   - Claude 3 Opus/Sonnet
   - Llama 3.1 70B (local via Ollama)

4. **Prompt b√°sico:**
   ```
   Act√∫a como experto en vermicompostaje dom√©stico. Genera chunks 
   parafraseados (en tus palabras) del siguiente contenido, siguiendo 
   el esquema JSON:

   [Pegar el prompt template completo de generate_cards_local.md]
   [Pegar el contenido del MD generado]
   ```

5. **Guarda la salida:** `chunks_tu_paper.jsonl`

**Salida esperada:** Archivo JSONL con 10-50 chunks seg√∫n el tama√±o del documento.

### Fase 3: Validaci√≥n Local (Pre-PR)

**Objetivo:** Asegurar que los chunks cumplen el esquema y criterios de calidad.

```bash
# Validaci√≥n de esquema (obligatoria)
python scripts/chunking/validate_chunks.py \
  --file chunks_tu_paper.jsonl \
  --mode schema

# Validaci√≥n sem√°ntica (opcional pero recomendada)
python scripts/chunking/validate_chunks.py \
  --file chunks_tu_paper.jsonl \
  --mode semantic \
  --sample 0.3

```

**Criterios de calidad m√≠nimos:**
- `confidence_score` ‚â• 0.70
- Campos obligatorios completos (ver `docs/DATA_SCHEMA.md`)
- Parafraseo claro (no copia textual)
- Atribuci√≥n (`source_document`, `citations`)

### Fase 4: Contribuci√≥n al Repositorio (PR)

**Objetivo:** Integrar tus chunks al dataset principal.

```bash
# 1. Mover el archivo validado al dataset
cp chunks_tu_paper.jsonl dataset/chunks_enriched/

# 2. Crear rama de feature
git checkout -b feature/add-chunks-nombre-descriptivo

# 3. Agregar solo el archivo JSONL
git add dataset/chunks_enriched/chunks_tu_paper.jsonl

# 4. Commit descriptivo
git commit -m "feat: Add chunks from [T√≠tulo/DOI de la fuente]

- Categor√≠a: [BIO/PROC/MAT/OPER/PROD]
- N√∫mero de chunks: [N]
- Fuente: [DOI o URL]
- Reliability: [verified/high/medium/low]"

# 5. Push a tu fork
git push origin feature/add-chunks-nombre-descriptivo

# 6. Abrir Pull Request en GitHub
```

**Descripci√≥n del PR debe incluir:**
- Fuente completa (t√≠tulo, autores, a√±o, DOI/URL)
- Categor√≠a(s) cubiertas
- N√∫mero de chunks agregados
- Si la fuente es CC-BY u open access (mencionar licencia)

### Fase 5: Revisi√≥n Autom√°tica (CI/CD)

Una vez abierto el PR, se realizar√°:

1. **Revisi√≥n Manual** (por ahora)
   - Un maintainer verificar√° campos obligatorios
   - Comprobar√° que el parafraseo es adecuado
   - Validar√° atribuci√≥n y fuentes

**Nota:** Los workflows autom√°ticos de CI/CD (GitHub Actions) est√°n planificados para futuras versiones. Por ahora, la validaci√≥n es manual despu√©s de ejecutar `validate_chunks.py` localmente.

**Si todo est√° bien:** El maintainer fusionar√° el PR.
**Si hay problemas:** Recibir√°s feedback para corregir.

---

## üéØ Tipos de Contribuciones

### 1. Agregar Nuevas Fuentes (Chunks)

**Prioridad:** ALTA - Necesitamos cubrir las 5 categor√≠as principales.

**Fuentes deseadas:**
- Papers de vermicompostaje dom√©stico (Eisenia fetida/andrei)
- Manuales t√©cnicos (FAO, INTA, universidades)
- Estudios de par√°metros (pH, C:N, humedad, temperatura)
- Gu√≠as de materiales org√°nicos y restricciones
- Documentos sobre producto final (humus, lixiviados)

**Template de Issue:** Usa [`.github/ISSUE_TEMPLATE/new_source.md`](.github/ISSUE_TEMPLATE/new_source.md)

### 2. Mejorar Chunks Existentes

**Ejemplos:**
- Corregir errores sem√°nticos
- Mejorar parafraseo
- Actualizar `confidence_score` con nueva evidencia
- Documentar conflictos entre fuentes

**Template de Issue:** Usa [`.github/ISSUE_TEMPLATE/dataset_improvement.md`]

### 3. Mejorar Herramientas (Scripts)

**√Åreas de mejora:**
- Optimizar `adaptive_converter.py` para casos complejos (tablas multi-p√°gina)
- Mejorar validaciones sem√°nticas en `validate_chunks.py`
- Desarrollar script de generaci√≥n automatizada de chunks
- Implementar sistema de embeddings y b√∫squeda vectorial

### 4. Documentaci√≥n

**√Åreas que necesitan cobertura:**
- Ejemplos de uso del dataset con LanceDB
- Gu√≠a de fine-tuning de LLMs con el dataset
- FAQ sobre el dominio (vermicompostaje)
- Tutoriales en video

---

## üìä M√©tricas de Calidad

Antes de abrir un PR, verifica:

### Esquema
- [ ] Todos los campos obligatorios presentes
- [ ] Tipos de datos correctos (string, float, bool, array)
- [ ] Enumeraciones v√°lidas (ver `docs/DATA_SCHEMA.md`)

### Contenido
- [ ] `source_field` parafraseado (NO copia textual)
- [ ] `confidence_score` ‚â• 0.70
- [ ] `source_document` es DOI/URL rastreable
- [ ] `citations` incluye referencias completas
- [ ] `keywords` relevantes (max 8)
- [ ] `entities` correctamente identificadas (max 10)

### Cobertura
- [ ] Cubre al menos 1 subcategor√≠a con <70% de cobertura
- [ ] No duplica contenido existente (similitud <0.85)

---

## üõ°Ô∏è Checklist Pre-PR

Antes de enviar tu Pull Request:

- [ ] Le√≠ y acepto la pol√≠tica BYOS (`BYOS_POLICY.md`)
- [ ] Mi archivo JSONL pas√≥ `validate_chunks.py --mode schema`
- [ ] Inclu√≠ atribuci√≥n clara en `source_document`
- [ ] Parafr ase√© el contenido (no es copia verbatim)
- [ ] NO estoy subiendo PDFs ni Markdowns directos
- [ ] Mi commit message sigue el formato convencional
- [ ] La descripci√≥n del PR incluye fuente completa

---

## ü§ù C√≥digo de Conducta

- **Respeto:** Trata a otros contribuidores con respeto
- **Colaboraci√≥n:** Compartimos el mismo objetivo
- **Transparencia:** Documenta fuentes y metodolog√≠a
- **Calidad sobre cantidad:** Mejor pocos chunks de calidad que muchos deficientes

---

## üí¨ ¬øDudas?

- **Consulta la documentaci√≥n:** [`docs/`](docs/)
- **Revisa Issues existentes:** [GitHub Issues](https://github.com/lizkarol/vermi-academic-rag/issues)
- **Abre una Discussion:** Para preguntas generales
- **Contacta maintainers:** Ver CODEOWNERS

---

¬°Gracias por contribuir a un recurso abierto, legal y de alta calidad para la comunidad! üå±
