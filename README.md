# Vermi Academic RAG Dataset

![GitHub License](https://img.shields.io/badge/License-MIT-blue.svg)
![BYOS Policy](https://img.shields.io/badge/Content%20Policy-BYOS-important.svg)
![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/lizkarol/vermi-academic-rag/validate-dataset.yml?branch=main&label=Dataset%20Validation)

Un repositorio p√∫blico y colaborativo para crear un dataset de Retrieval-Augmented Generation (RAG) de alta calidad, especializado en **vermicompostaje dom√©stico** y construido bajo una estricta pol√≠tica legal-safe.

---

## üß† Filosof√≠a: "El Motor, no la Gasolina"

Este proyecto opera bajo un principio fundamental: **proporcionamos las herramientas (el motor), pero no los datos brutos con copyright (la gasolina)**.

### ¬øPor qu√© BYOS (Bring Your Own Sources)?

Debido a las restricciones de derechos de autor de los papers acad√©micos y manuales t√©cnicos, **no podemos alojar directamente los PDFs ni sus textos extra√≠dos**. En su lugar, hemos desarrollado un flujo de trabajo que permite a la comunidad:

1. **Procesar documentos localmente** (en tu m√°quina, de forma privada)
2. **Generar chunks parafraseados** (con tus propias palabras, usando LLMs)
3. **Contribuir solo los datos estructurados** (que no infringen copyright)

### üîß El "Motor" que proporcionamos:

#### **Prioridad #1: Conversi√≥n PDF ‚Üí Markdown con Sistema Adaptativo**
*   **`scripts/conversion/adaptive_converter.py`**: Sistema inteligente que selecciona la mejor estrategia seg√∫n tipo de PDF
    - **PDFs nativos** (texto seleccionable): pdfplumber (‚ö° 5-10s)
    - **PDFs escaneados** (imagen): marker-pdf + EasyOCR + GPU (üî¨ 5-7min)
    - **PDFs mixtos** (h√≠bridos): docling con detecci√≥n autom√°tica (‚öñÔ∏è 30-60s)
    - Tracking con SQLite (detecci√≥n de duplicados por SHA-256)
    - **Post-procesamiento con normalizaci√≥n** (jerarqu√≠a, limpieza, fusi√≥n de l√≠neas)
    - Validaci√≥n opcional con Ollama gemma3:12b (local, BYOS)

#### Herramientas de Validaci√≥n:
*   **`scripts/chunking/validate_chunks.py`**: Valida esquema de chunks (modos: schema, semantic, coverage)

#### Documentaci√≥n y Plantillas:
*   **Esquemas de datos**: Estructura clara y validable para chunks
*   **Gu√≠as de contribuci√≥n**: Flujo paso a paso con ejemplos
*   **Templates de prompts**: Para generar chunks con LLMs

### ‚õΩ La "Gasolina" que t√∫ pones:

Los papers acad√©micos, manuales t√©cnicos o documentos de vermicompostaje a los que tienes **acceso legal**.

## üöÄ Quickstart: C√≥mo Contribuir

### Requisitos Previos

- **Python 3.11+**
- **macOS, Ubuntu, o Windows** (soporte multi-plataforma)
- **Acceso a un LLM** (Gemini, GPT-4, Claude, o modelos locales v√≠a Ollama)

### Pasos para tu Primera Contribuci√≥n:

#### 1. **Setup Inicial**
```bash
# Clonar el repositorio
git clone https://github.com/lizkarol/vermi-academic-rag.git
cd vermi-academic-rag

# Ejecutar setup autom√°tico (recomienda)
./setup.sh

# O instalar manualmente (ver scripts/requirements.txt para detalles)
pip install -r scripts/requirements.txt
```

#### 2. **Conversi√≥n PDF ‚Üí Markdown (SISTEMA ADAPTATIVO)**
El sistema detecta autom√°ticamente el tipo de PDF y aplica la estrategia √≥ptima.

```bash
# 1. Detectar tipo de PDF (r√°pido, < 1s)
python scripts/conversion/pdf_type_detector.py paper.pdf

# 2. Conversi√≥n autom√°tica (selecciona estrategia)
python scripts/conversion/adaptive_converter.py paper.pdf

# 3. Con validaci√≥n Ollama (opcional, +10-30s)
python scripts/conversion/adaptive_converter.py paper.pdf --ollama

# 4. Sin normalizaci√≥n de markdown (solo conversi√≥n cruda)
python scripts/conversion/adaptive_converter.py paper.pdf --no-normalize

# 5. Forzar estrategia espec√≠fica (debug)
python scripts/conversion/adaptive_converter.py paper.pdf --strategy scanned

# 6. Forzar reconversi√≥n (ignorar duplicados)
python scripts/conversion/adaptive_converter.py paper.pdf --force
```

**Salida:** 
- `sources_local/converted/paper.md` (Markdown generado y normalizado)
- `sources_local/metadata/conversion_tracker.db` (Tracking SQLite)
- `sources_local/reports/paper_normalization.json` (Reporte de cambios aplicados)
- `sources_local/reports/paper_validation.json` (Si usas --ollama)

**Performance:**
- PDFs nativos: ~5-10 segundos (pdfplumber)
- PDFs escaneados: ~5-7 minutos con GPU (marker-pdf + OCR)
- PDFs mixtos: ~30-60 segundos (docling)

**Caracter√≠sticas avanzadas:**
- ‚úÖ Detecci√≥n de duplicados por hash SHA-256
- ‚úÖ **Post-procesamiento de normalizaci√≥n autom√°tico** (jerarqu√≠a, limpieza, fusi√≥n)
- ‚úÖ **Soporte multi-formato**: APA, Vancouver, IEEE, Chicago, Harvard, MLA, ISO
- ‚úÖ **Detecci√≥n inteligente**: Decimal, Romano, Letras, Palabras clave
- ‚úÖ Validaci√≥n de fidelidad con gemma3:12b (Ollama)
- ‚úÖ Extracci√≥n de tablas con pdfplumber
- ‚úÖ Hardware detection autom√°tico (MPS/CUDA/CPU)
- ‚úÖ Tracking en SQLite con estad√≠sticas

**Ver m√°s:** [`docs/CONVERSION_SYSTEM.md`](docs/CONVERSION_SYSTEM.md)

#### 3. **Generar Chunks Parafraseados (LLM - Manual)**
Usa el Markdown generado para crear chunks en tus propias palabras.

**Pasos:**
1. Abre `scripts/chunking/generate_cards_local.md` (plantilla de prompt)
2. Copia el contenido del Markdown generado en el paso 2
3. Usa tu LLM favorito (Gemini, GPT-4, Claude, etc.) con la plantilla
4. El LLM generar√° chunks en formato JSONL
5. Guarda el resultado como `chunks_tu_paper.jsonl`

**Resultado:** Archivo JSONL con chunks parafraseados

#### 4. **Validar Localmente**
Antes de hacer PR, valida que tus chunks cumplen el esquema:

```bash
python scripts/chunking/validate_chunks.py \
  --file chunks_tu_paper.jsonl \
  --mode schema
```

Si todo est√° OK, copia el archivo a `dataset/chunks_enriched/`

#### 5. **Contribuir al Repositorio**
```bash
git checkout -b feature/add-chunks-nombre-paper
git add dataset/chunks_enriched/chunks_tu_paper.jsonl
git commit -m "feat: Add chunks from [Nombre Paper/Manual]"
git push origin feature/add-chunks-nombre-paper
```

Abre un Pull Request describiendo:
- Fuente del documento (DOI/URL si es posible)
- Categor√≠a cubierta (BIO/PROC/MAT/OPER/PROD)
- N√∫mero de chunks agregados

**‚ö†Ô∏è Recuerda:** NO subas PDFs ni Markdowns directos. Solo los chunks parafraseados en JSONL.

## üéØ Contexto del Proyecto: VermiKhipu

Este dataset es parte del proyecto **VermiKhipu**, un sistema de vermicompostaje dom√©stico asistido por IA que opera 100% offline con interacci√≥n por voz en espa√±ol.

### Dominio del Conocimiento

El dataset se enfoca en **vermicompostaje dom√©stico** (escala 1-5 kg/semana), cubriendo:

- **ü¶† BIOLOG√çA**: Especies de lombrices (Eisenia fetida/andrei), fisiolog√≠a, comportamiento
- **‚öôÔ∏è PROCESO**: Par√°metros (pH, C:N, humedad, temperatura), cin√©tica de degradaci√≥n
- **ÔøΩ MATERIALES**: Residuos org√°nicos, materiales secos, clasificaci√≥n y restricciones
- **üéÆ OPERACI√ìN**: Control de h√°bitat, intervenciones, calibraci√≥n de actuadores
- **üå± PRODUCTO**: Humus maduro, lixiviados, aplicaciones y dosificaci√≥n

### Cobertura Actual

| Categor√≠a | Chunks Objetivo | Estado | Prioridad |
|-----------|----------------|--------|-----------|
| BIOLOG√çA  | 40-60          | üî¥ 0%  | ALTA      |
| PROCESO   | 60-80          | üî¥ 0%  | CR√çTICA   |
| MATERIALES| 50-70          | üî¥ 0%  | ALTA      |
| OPERACI√ìN | 30-40          | üî¥ 0%  | MEDIA     |
| PRODUCTO  | 20-30          | üî¥ 0%  | MEDIA     |

**Meta Fase 1 (MVP):** 150-200 chunks cubriendo 70% de la taxonom√≠a

---

## ÔøΩ Documentaci√≥n

- **[INSTALLATION.md](INSTALLATION.md)** - Instalaci√≥n paso a paso (macOS/Windows)
- **[CONTRIBUTING.md](CONTRIBUTING.md)** - C√≥mo contribuir al dataset
- **[ROADMAP.md](ROADMAP.md)** - Plan de desarrollo del proyecto
- **[BYOS_POLICY.md](BYOS_POLICY.md)** - Pol√≠tica de contenido legal-safe
- **[docs/DOMAIN_KNOWLEDGE.md](docs/DOMAIN_KNOWLEDGE.md)** - Taxonom√≠a y dominio
- **[docs/DATA_SCHEMA.md](docs/DATA_SCHEMA.md)** - Esquema de datos detallado
- **[docs/MARKDOWN_NORMALIZATION.md](docs/MARKDOWN_NORMALIZATION.md)** - Sistema de post-procesamiento

---

## üìú Licencia y Pol√≠tica de Contenido

*   El **c√≥digo** de este repositorio est√° bajo **Licencia MIT**.
*   Las **contribuciones de datos** se rigen por nuestra **[Pol√≠tica BYOS](BYOS_POLICY.md)**, que debes leer y aceptar.

---

## üõ†Ô∏è Estado Actual (Fase 0)

**Funcional:**
- ‚úÖ Conversi√≥n PDF‚ÜíMarkdown con sistema adaptativo (`adaptive_converter.py`)
- ‚úÖ Post-procesamiento de normalizaci√≥n (`markdown_normalizer.py`)
- ‚úÖ Validaci√≥n de esquema (`validate_chunks.py`)
- ‚úÖ Plantilla de generaci√≥n de chunks (manual con LLM)
- ‚úÖ Documentaci√≥n completa y estructura organizada

**En Desarrollo:**
- üî® Generaci√≥n automatizada de chunks
- üî® Sistema de embeddings y b√∫squeda vectorial
- üî® Pruebas RAG autom√°ticas
- üî® CI/CD workflows

**Pr√≥ximo Hito:** Primer ciclo de ingesta MVP (10-20 chunks reales)

---

¬°Gracias por considerar contribuir a un recurso abierto, legal y de alta calidad para la comunidad de IA!
