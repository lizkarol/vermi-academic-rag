# Estado del Proyecto - Vermi Academic RAG

**√öltima actualizaci√≥n:** 2025-11-03  
**Fase actual:** Fase 0 (Consolidaci√≥n) - ‚úÖ **100% COMPLETA**

---

## ‚úÖ Funcionalidad Implementada y Probada

### Core (Cr√≠tico)

- **Conversi√≥n PDF‚ÜíMarkdown Adaptativa**
  - Script: `scripts/conversion/adaptive_converter.py`
  - Tecnolog√≠a: Sistema adaptativo (pdfplumber para nativos, marker-pdf + EasyOCR para escaneados)
  - Detecci√≥n autom√°tica de tipo de PDF con `PDFTypeDetector`
  - Sistema de perfiles JSON personalizables (`config/profiles/`)
  - Detecci√≥n autom√°tica de perfiles con `ProfileDetector`
  - Normalizaci√≥n avanzada de Markdown con `MarkdownNormalizer`
  - Tracking en SQLite con `ConversionTracker`
  - Estado: ‚úÖ **Completamente funcional**
  - Uso: `python scripts/conversion/adaptive_converter.py paper.pdf --profile academic_apa`

### Validaci√≥n

- **Validaci√≥n de esquema**
  - Script: `scripts/chunking/validate_chunks.py`
  - Modos: schema, semantic (placeholder), coverage (placeholder)
  - Estado: ‚úÖ Modo schema funcional
  - Uso: `python scripts/chunking/validate_chunks.py --file chunks.jsonl --mode schema`

### Generaci√≥n de Chunks

- **Template manual para LLMs**
  - Archivo: `scripts/chunking/generate_cards_local.md`
  - Estado: ‚úÖ Funcional (proceso manual)
  - Uso: Copiar template ‚Üí LLM web (Gemini/GPT-4/Claude) ‚Üí Generar JSONL

### Documentaci√≥n

- ‚úÖ README.md completo y actualizado
- ‚úÖ INSTALLATION.md (macOS/Windows)
- ‚úÖ CONTRIBUTING.md con flujo 5 fases
- ‚úÖ ROADMAP.md detallado por fases
- ‚úÖ BYOS_POLICY.md
- ‚úÖ docs/DOMAIN_KNOWLEDGE.md (taxonom√≠a completa)
- ‚úÖ docs/DATA_SCHEMA.md
- ‚úÖ docs/CONVERSION_SYSTEM.md (arquitectura del sistema de conversi√≥n)
- ‚úÖ docs/ADAPTIVE_CONVERSION.md (gu√≠a t√©cnica detallada)
- ‚úÖ docs/OCR_TABLES_ROADMAP.md (roadmap de OCR y tablas)
- ‚úÖ config/profiles/README.md (sistema de perfiles JSON)
- ‚úÖ READMEs en subdirectorios de scripts
- ‚úÖ **Toda la documentaci√≥n alineada con el c√≥digo actual**

### Infraestructura

- ‚úÖ `.gitignore` exhaustivo (protecci√≥n BYOS)
- ‚úÖ `.env.example` con configuraci√≥n completa
- ‚úÖ `setup.sh` script de instalaci√≥n automatizada
- ‚úÖ Estructura de directorios organizada
- ‚úÖ `sources_local/` (directorio local para PDFs, reemplaza `sources/`)
- ‚úÖ GitHub Actions workflows actualizados (manual trigger para prevenir fallos)
- ‚úÖ Tests deshabilitados temporalmente (pendientes de datos reales)

---

## üöß En Desarrollo (No Implementado A√∫n)

### Fase 2 (Planificado para Q1 2026)

- **Generaci√≥n Automatizada de Chunks**
  - Script: `scripts/chunking/generate_cards.py`
  - Integraci√≥n con Ollama para LLMs locales
  - Batch processing de m√∫ltiples documentos

- **Sistema de Embeddings**
  - Script: `scripts/embeddings/compute_embeddings.py`
  - Vectorizaci√≥n con `embeddinggemma:300m`
  - Generaci√≥n de archivos `.parquet` con vectores

- **Base de Datos Vectorial**
  - LanceDB con √≠ndices IVF_HNSW_SQ + FTS
  - B√∫squeda h√≠brida (vector + keyword)
  - Reranking con RRF

- **Detecci√≥n de Duplicados**
  - Script: `scripts/chunking/merge_redundant.py`
  - Similitud coseno para detectar redundancia
  - Sugerencias de fusi√≥n

- **Pruebas RAG**
  - Script: `scripts/testing/test_retrieval.py`
  - Test suite con queries sint√©ticas
  - M√©tricas de calidad (precision, recall, relevance)

- **CI/CD Workflows**
  - `.github/workflows/validate-dataset.yml` (‚úÖ Actualizado - manual trigger)
  - `.github/workflows/test-rag.yml` (‚úÖ Actualizado - manual trigger)
  - `.github/workflows/publish-release.yml` (‚úÖ Actualizado - manual trigger)
  - **Nota:** Workflows cambiados a `workflow_dispatch` para evitar fallos autom√°ticos hasta que el dataset est√© poblado

### Fase 3 (Planificado para Q2 2026)

- Release v1.0 oficial
- Dataset completo (200-280 chunks)
- Sistema RAG funcional end-to-end
- Ejemplos de integraci√≥n

---

## üìä Dataset Actual

**Estado:** Solo chunks de ejemplo (placeholder)

| Archivo | Chunks | Estado |
|---------|--------|--------|
| `chunks_enriched_v1.0.jsonl` | 1 | Ejemplo/template |
| `chunks_enriched_v1.0.feather` | 0 | Vac√≠o |

**Pr√≥ximo objetivo:** Primer ciclo de ingesta real (10-20 chunks de un documento acad√©mico)

---

## üéØ Cobertura por Categor√≠a

| Categor√≠a | Objetivo | Actual | % |
|-----------|----------|--------|---|
| BIOLOG√çA (BIO) | 40-60 | 0 | 0% |
| PROCESO (PROC) | 60-80 | 0 | 0% |
| MATERIALES (MAT) | 50-70 | 0 | 0% |
| OPERACI√ìN (OPER) | 30-40 | 0 | 0% |
| PRODUCTO (PROD) | 20-30 | 0 | 0% |
| **TOTAL** | **200-280** | **0** | **0%** |

---

## üîÑ Flujo de Trabajo Actual (Funcional)

```
1. Usuario tiene PDF acad√©mico (local)
   ‚Üì
2. Ejecuta adaptive_converter.py
   ‚Üì
3. Obtiene Markdown (sources_local/markdown_outputs/)
   ‚Üì
4. Abre generate_cards_local.md (template)
   ‚Üì
5. Usa LLM web (Gemini/GPT-4/Claude) manualmente
   ‚Üì
6. LLM genera chunks.jsonl
   ‚Üì
7. Ejecuta validate_chunks.py --mode schema
   ‚Üì
8. Si OK: Copia a dataset/chunks_enriched/
   ‚Üì
9. Crea Pull Request
   ‚Üì
10. Revisi√≥n manual por maintainer
   ‚Üì
11. Merge si todo est√° correcto
```

**Tiempo estimado:** 30-60 minutos por documento (proceso manual)

---

## üõ†Ô∏è Dependencias Cr√≠ticas

### Instaladas y Funcionales

- Python 3.11+
- PyTorch (backend para marker)
- marker-sdk (conversi√≥n PDF‚ÜíMD)
- pandas, pyarrow (manejo de datos)
- pytest (testing del esquema)

### Opcionales (Fase 2+)

- Ollama (LLMs locales)
- LanceDB (base de datos vectorial)
- DuckDB (m√©tricas y telemetr√≠a)

---

## üìù Pr√≥ximos Pasos Inmediatos

### ‚úÖ Fase 0 Completada (2025-11-03)

1. [x] **Consolidaci√≥n de arquitectura**
   - Sistema de perfiles JSON implementado y documentado
   - Toda la documentaci√≥n alineada con el c√≥digo real
   - Referencias obsoletas eliminadas (`convert_pdf_local.py`, `convert_pdf_robust.py`)
   - Directorio `sources/` renombrado a `sources_local/`
   - Tests rotos deshabilitados temporalmente
   - Workflows de CI actualizados a manual trigger

2. [ ] **Primer test end-to-end** (Pr√≥ximo paso)
   - Conseguir un PDF acad√©mico de vermicompostaje (CC-BY o acceso legal)
   - Ejecutar flujo completo: PDF ‚Üí MD ‚Üí Chunks ‚Üí Validaci√≥n
   - Documentar problemas encontrados

3. [ ] **Primer chunk real** (Pr√≥ximo paso)
   - Generar 5-10 chunks reales de un documento
   - Validar con `validate_chunks.py`
   - Agregar a `dataset/chunks_enriched/`

### Para Iniciar Fase 1 (Pr√≥ximas 2 Semanas)

1. [ ] Seleccionar 5-10 fuentes acad√©micas (prioritarias: PROCESO y BIOLOG√çA)
2. [ ] Procesar cada fuente siguiendo el flujo documentado
3. [ ] Meta: 50-100 chunks validados
4. [ ] Documentar casos edge y problemas de conversi√≥n

---

## üö® Limitaciones Conocidas

### T√©cnicas

- Conversi√≥n PDF‚ÜíMD puede fallar con PDFs muy complejos (tablas multi-p√°gina)
- Validaci√≥n sem√°ntica no implementada (solo schema)
- No hay detecci√≥n autom√°tica de duplicados
- Proceso de generaci√≥n de chunks es 100% manual

### De Proceso

- Sin CI/CD autom√°tico (validaci√≥n manual)
- Sin sistema de embeddings (b√∫squeda vectorial futura)
- Sin pruebas RAG autom√°ticas
- Sin m√©tricas de calidad del dataset

### De Cobertura

- Dataset vac√≠o (0 chunks reales)
- Sin validaci√≥n con dominio experto
- Sin casos de uso reales probados

---

## üí° Lecciones Aprendidas

1. **Simplicidad primero:** Mejor documentar lo que funciona que prometer lo que no existe
2. **BYOS es cr√≠tico:** La estructura debe proteger contra subidas accidentales de copyright
3. **Manual es OK:** Automatizaci√≥n puede esperar a Fase 2; calidad es m√°s importante
4. **Documentaci√≥n clara:** Evitar confusi√≥n entre "planificado" y "funcional"

---

## üéì Para Nuevos Contribuidores

**Empieza aqu√≠:**
1. Lee [INSTALLATION.md](INSTALLATION.md)
2. Ejecuta `./setup.sh`
3. Prueba conversi√≥n con un PDF de prueba
4. Lee [docs/DOMAIN_KNOWLEDGE.md](docs/DOMAIN_KNOWLEDGE.md) para entender la taxonom√≠a
5. Genera tus primeros chunks siguiendo [CONTRIBUTING.md](CONTRIBUTING.md)

**Lo que necesitas saber:**
- Solo `adaptive_converter.py` y `validate_chunks.py` est√°n implementados
- La generaci√≥n de chunks es manual con LLMs web
- No hay sistema de embeddings a√∫n
- El dataset est√° vac√≠o, ¬°eres de los primeros contribuidores!

---

**Este documento se actualizar√° al completar cada fase del proyecto.**
